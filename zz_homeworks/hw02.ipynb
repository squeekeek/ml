{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN\n",
    "\n",
    "This class contains our simple artificial neural network (ANN) instance. It has the following customizable parameters\n",
    "\n",
    "Initialization:\n",
    "- `rand_mean` - mean of the normal distribution used to initialize W and B\n",
    "- `rand_std` - STD of the normal distribution used to initialize W and B\n",
    "\n",
    "Training/Learning:\n",
    "- `learn_rate` - learn rate as a double between 0 and 1\n",
    "- `batch_size` - size of each data pushed through forward propagation. \n",
    "- `num_epoch` - number of data set passes through the ANN.\n",
    "- `w_shapes` - shape of each weight matrix W, like (out_dim, in_dim), written from left to right. `w_shapes[i - 1][0] == w_shapes[i][1]`\n",
    "- `b_shapes` - shape of each bias vector B with the same count as `w_shapes`. always a column vector such that `w_shapes[i][0] == b_shapes[i][0]`\n",
    "- `activ_funcs` - activation functions for each layer with the same count as `w_shapes`. each function always accepts only one parameter\n",
    "- `activ_funcs_dx` - equivalent derivative functions of the activation functions in `activ_funcs`. each function always accepts only one parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ANN():\n",
    "    def __init__(self): \n",
    "        self.rand_mean = 0\n",
    "        self.rand_std = 0.01\n",
    "\n",
    "        self.learn_rate = 0.1\n",
    "        self.batch_size = 1\n",
    "        self.num_epoch = 20\n",
    "\n",
    "        self.w_shapes = [(64, self.batch_size), (64, 64), (self.batch_size, 64)]\n",
    "        #self.b_shapes = [(64, 1), (64, 1), (self.batch_size, 1)]\n",
    "        self.activ_funcs = [self.relu, self.relu, self.sigmoid]\n",
    "        self.activ_funcs_dx = [self.relu_dx, self.relu_dx, self.sigmoid_dx]\n",
    "\n",
    "        self.wx = [np.random.normal(self.rand_mean, self.rand_std, x) for x in self.w_shapes]\n",
    "        #self.bx = [np.random.normal(self.rand_mean, self.rand_std, x) for x in self.b_shapes]\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_dx(self, x):\n",
    "        return self.sigmoid(x) * (1 - self.sigmoid(x))\n",
    "\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def relu_dx(self, x):\n",
    "        # NOTE: No derivative at x = 0\n",
    "        # Convention is dx at 0 is zero\n",
    "        relu_scalar = lambda x: 1 if x > 0 else 0\n",
    "        relu_vec = np.vectorize(relu_scalar)\n",
    "\n",
    "        return relu_vec(x)\n",
    "    \n",
    "    def save_data(self, data, filename=\"hw02_data.txt\"):\n",
    "        np.savetxt(filename, data, fmt=\"%s\")\n",
    "    \n",
    "    def init_params(self):\n",
    "        self.wx = [np.random.normal(self.rand_mean, self.rand_std, x) for x in self.w_shapes]\n",
    "        #self.bx = [np.random.normal(self.rand_mean, self.rand_std, x) for x in self.b_shapes]\n",
    "    \n",
    "    def forward_prop(self, in_data):\n",
    "        ax = [[] for i in range(len(self.wx))]\n",
    "        zx = [[] for i in range(len(self.wx))]\n",
    "        \n",
    "        for i in range(len(self.wx)):\n",
    "            past_val = in_data if i == 0 else ax[i - 1]\n",
    "        \n",
    "            zx[i] = self.wx[i].dot(past_val)\n",
    "            ax[i] = self.activ_funcs[i](zx[i])\n",
    "        \n",
    "        return zx, ax\n",
    "    \n",
    "    def backward_prop(self, zx, ax, in_data, answer_data):\n",
    "        '''\n",
    "        dWx = [[] for i in range(len(self.w_shapes))]\n",
    "\n",
    "        error = -2 * (out_actual - ax[2]) * self.sigmoid_dx(zx[2])\n",
    "        dWx[2] = error.dot(ax[1].T)\n",
    "\n",
    "        error = self.wx[2].T.dot(error) * self.relu_dx(zx[1])\n",
    "        dWx[1] = error.dot(ax[0].T)\n",
    "\n",
    "        error = self.wx[1].T.dot(error) * self.relu_dx(zx[0])\n",
    "        dWx[0] = error.dot(in_data.T)\n",
    "\n",
    "        for i, v in enumerate(dWx):\n",
    "            self.wx[i] -= self.learn_rate * v\n",
    "        '''\n",
    "        \n",
    "        dWx = [[] for i in range(len(self.w_shapes))]\n",
    "        error = 0\n",
    "        \n",
    "        pred_data = ax[-1]\n",
    "        \n",
    "        # Determine change\n",
    "        for i in reversed(range(len(self.wx))):\n",
    "            past_cost = (-2 * (answer_data - pred_data) / ax[-1].shape[0]) if i == len(self.wx) - 1 else self.wx[i + 1].T.dot(error)\n",
    "            activ_dx = self.activ_funcs_dx[i](zx[i])\n",
    "            f_val = in_data if i == 0 else ax[i - 1]\n",
    "            \n",
    "            error = past_cost * activ_dx\n",
    "            dWx[i] = error.dot(f_val.T)\n",
    "        \n",
    "        # Recalibrate weights\n",
    "        for i in range(len(self.wx)):\n",
    "            self.wx[i] -= self.learn_rate * dWx[i]\n",
    "        \n",
    "        return self.wx\n",
    "    \n",
    "    def predict(self, data_in):\n",
    "        num_iters = int(len(data_in) / self.batch_size)\n",
    "        out_pred_vec = []\n",
    "        \n",
    "        for each_batch_idx in range(num_iters):\n",
    "            batch_start_idx = self.batch_size * each_batch_idx\n",
    "            \n",
    "            in_data = np.atleast_2d(data_in[batch_start_idx:batch_start_idx + self.batch_size]).T\n",
    "            \n",
    "            _, new_ax = self.forward_prop(in_data)\n",
    "            out_pred = new_ax[-1]\n",
    "            out_pred_vec.append(out_pred)\n",
    "        \n",
    "        return out_pred_vec\n",
    "    \n",
    "    def train(self, data_in, data_out, clear_params=False):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        if clear_params:\n",
    "            self.init_params()\n",
    "            \n",
    "        # Training workflow\n",
    "        num_iters = int(len(data_in) / self.batch_size)\n",
    "        print(f'Running training for {self.num_epoch} epoch(s), {self.batch_size} batch(es), and {num_iters} iteration(s)')\n",
    "\n",
    "        for each_epoch_idx in range(self.num_epoch):\n",
    "            mse_vec = []\n",
    "            \n",
    "            # Randomize data each epoch\n",
    "            #rand_idxs = np.random.permutation(data_in.size)\n",
    "            #data_in = data_in[rand_idxs]\n",
    "            #data_out = data_out[rand_idxs]\n",
    "            \n",
    "            for each_batch_idx in range(num_iters):\n",
    "                batch_start_idx = self.batch_size * each_batch_idx\n",
    "\n",
    "                in_data = np.atleast_2d(data_in[batch_start_idx:batch_start_idx + self.batch_size]).T\n",
    "                out_actual = np.atleast_2d(data_out[batch_start_idx:batch_start_idx + self.batch_size]).T\n",
    "\n",
    "                # Forward\n",
    "                zx, ax = self.forward_prop(in_data)\n",
    "\n",
    "                # Back - MSE as loss function\n",
    "                self.backward_prop(zx, ax, in_data, out_actual)\n",
    "                \n",
    "                mse_vec.append(ax[2] - out_actual)\n",
    "                \n",
    "                if (each_batch_idx % (num_iters / 10)) == 0:\n",
    "                    print(f'At idx: {in_data} -> {out_actual}T vs {ax[-1]}P')\n",
    "            \n",
    "                #print(in_data, out_actual, ax[2])\n",
    "            \n",
    "            mse_val = np.sum(np.square(mse_vec)) / len(data_in)\n",
    "            \n",
    "            print(f'Epoch {each_epoch_idx + 1} MSE: {mse_val}')\n",
    "            print(f'Min Weight Vals: {np.min(self.wx[0]), np.min(self.wx[1]), np.min(self.wx[2])}')\n",
    "            print(f'Max Weight Vals: {np.max(self.wx[0]), np.max(self.wx[1]), np.max(self.wx[2])}')\n",
    "            \n",
    "            #self.save_data(self.wx[0], filename=f\"wx0_{each_epoch_idx}.txt\")\n",
    "            #self.save_data(self.wx[1], filename=f\"wx1_{each_epoch_idx}.txt\")\n",
    "            #self.save_data(self.wx[2], filename=f\"wx2_{each_epoch_idx}.txt\")\n",
    "        \n",
    "        print(f'Training done in {round(time.time() - start_time, 2)}s!')\n",
    "\n",
    "    def test(self, data_in, data_out):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Test workflow\n",
    "        num_iters = int(len(data_in) / self.batch_size)\n",
    "        mse_vec = []\n",
    "        \n",
    "        out_pred = np.reshape(self.predict(data_in), data_in.shape)\n",
    "        mse_vec = data_out - out_pred\n",
    "        \n",
    "        mse_val = np.sum(np.square(mse_vec)) / len(data_in)\n",
    "        \n",
    "        print(f'Min Weight Vals: {np.min(self.wx[0]), np.min(self.wx[1]), np.min(self.wx[2])}')\n",
    "        print(f'Max Weight Vals: {np.max(self.wx[0]), np.max(self.wx[1]), np.max(self.wx[2])}')\n",
    "        print(f'Mean squared error: {mse_val}')\n",
    "        \n",
    "        print(f'Test done in {round(time.time() - start_time, 2)}s!')\n",
    "        \n",
    "        return out_pred\n",
    "    \n",
    "    def plot_test(self, data_in, data_out):\n",
    "        y_pred = self.test(data_in, data_out)\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.scatter(data_in, data_out, s=2)\n",
    "        plt.scatter(data_in, y_pred, s=6, c='r')\n",
    "        plt.xlabel('Sample Value')\n",
    "        plt.ylabel('Chance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init\n",
    "The following contains the routine to set-up the normal distribution data. It has the following customizable parameters.\n",
    "\n",
    "- `mean` - mean of the normal distribution where the input data will be sampled\n",
    "- `std` - STD of the normal distribution where the input data will be sampled\n",
    "- `num_samp` - number of samples to get from the normal distribution\n",
    "- `num_bins` - granularity of the output. the bins \n",
    "- `train_percent` - percent of the data that will be allocated as the training dataset. between 0 and 1\n",
    "\n",
    "The data sampling works by first getting `num_samp` samples from a normal distribution with mean `mean` and std `std`. Then, a histogram of the samples were generated - which are then sorted into `num_bins` bins. The histogram approximates a normal distribution, with each bin holding the value of the approximate probability density function (pdf) of the distribution such that the overall area is 1. The sample data are then labeled to determine which bin they belong. Then, they are shuffled once to ensure a randomized dataset split. The sample data is then split into training and test datasets from this shuffling.\n",
    "\n",
    "The input is the sample data at the distribution and the output is its pdf at that value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 5; STD: 1\n",
      "TotalData: 1000; NumTrain: 900; NumTest: 100\n"
     ]
    }
   ],
   "source": [
    "# Set initial inputs here\n",
    "mean = 5\n",
    "std = 1\n",
    "num_samp = 1000000\n",
    "num_bins = 1000\n",
    "train_percent = 0.9\n",
    "\n",
    "# Draw 1m samples from a 1D normal distribution with the given mean and std\n",
    "# From a sample, predict its chance [0, 1]\n",
    "norm_samp = np.random.normal(mean, std, (1, num_samp))\n",
    "norm_hist_out, norm_edges = np.histogram(norm_samp, bins=num_bins, density=True)\n",
    "norm_hist_idxs = np.digitize(norm_samp, norm_edges)\n",
    "\n",
    "# Pre-filter histogram idxs\n",
    "norm_hist_idxs -= 1\n",
    "norm_hist_idxs[norm_hist_idxs <= 0] = 0\n",
    "norm_hist_idxs[norm_hist_idxs >= norm_hist_out.size] = norm_hist_out.size - 1\n",
    "\n",
    "# Split dataset by shuffling the bins\n",
    "train_bound_idx = int(train_percent * norm_samp.size)\n",
    "rand_idxs = np.random.permutation(norm_samp.size) \n",
    "train_idxs, test_idxs = rand_idxs[:train_bound_idx], rand_idxs[train_bound_idx:]\n",
    "\n",
    "train_set_in, train_set_out = norm_samp[0][train_idxs], norm_hist_out[norm_hist_idxs[0][train_idxs]]\n",
    "test_set_in, test_set_out = norm_samp[0][test_idxs], norm_hist_out[norm_hist_idxs[0][test_idxs]]\n",
    "\n",
    "print(f'Mean: {mean}; STD: {std}')\n",
    "print(f'TotalData: {len(rand_idxs)}; NumTrain: {len(train_set_in)}; NumTest: {len(test_set_in)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "The following snippet trains a freshly-initialized ANN. The program prints every other batch since the training itself can get pretty long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running training for 3 epoch(s), 1 batch(es), and 900 iteration(s)\n",
      "At idx: [[6.37223739]] -> [[0.33775799]]T vs [[0.49992501]]P\n",
      "At idx: [[4.88750426]] -> [[0.33775799]]T vs [[0.49991731]]P\n",
      "At idx: [[5.73828744]] -> [[0.33775799]]T vs [[0.49986829]]P\n",
      "At idx: [[4.17618157]] -> [[0.84439498]]T vs [[0.49987873]]P\n",
      "At idx: [[5.10429331]] -> [[0.67551599]]T vs [[0.4998309]]P\n",
      "At idx: [[4.05661794]] -> [[0.50663699]]T vs [[0.49978933]]P\n",
      "At idx: [[3.88933506]] -> [[0.50663699]]T vs [[0.49972195]]P\n",
      "At idx: [[4.82082767]] -> [[0.33775799]]T vs [[0.49963248]]P\n",
      "At idx: [[4.3328766]] -> [[0.33775799]]T vs [[0.49965985]]P\n",
      "At idx: [[4.95772573]] -> [[0.84439498]]T vs [[0.4994983]]P\n",
      "Epoch 1 MSE: 0.05817015458259337\n",
      "Min Weight Vals: (-0.02353612347756324, -0.03940214243464389, -0.030089965136066182)\n",
      "Max Weight Vals: (0.03102791577766622, 0.039825005303173684, 0.03200058875980276)\n",
      "At idx: [[6.37223739]] -> [[0.33775799]]T vs [[0.49917377]]P\n",
      "At idx: [[4.88750426]] -> [[0.33775799]]T vs [[0.49914572]]P\n",
      "At idx: [[5.73828744]] -> [[0.33775799]]T vs [[0.49859797]]P\n",
      "At idx: [[4.17618157]] -> [[0.84439498]]T vs [[0.49861105]]P\n",
      "At idx: [[5.10429331]] -> [[0.67551599]]T vs [[0.49796763]]P\n",
      "At idx: [[4.05661794]] -> [[0.50663699]]T vs [[0.49652938]]P\n",
      "At idx: [[3.88933506]] -> [[0.50663699]]T vs [[0.49380462]]P\n",
      "At idx: [[4.82082767]] -> [[0.33775799]]T vs [[0.4921244]]P\n",
      "At idx: [[4.3328766]] -> [[0.33775799]]T vs [[0.49324518]]P\n",
      "At idx: [[4.95772573]] -> [[0.84439498]]T vs [[0.48763308]]P\n",
      "Epoch 2 MSE: 0.057855077950295083\n",
      "Min Weight Vals: (-0.02353612347756324, -0.03940214243464389, -0.10675800959637834)\n",
      "Max Weight Vals: (0.09273685300741474, 0.047695429724176355, 0.03200058875980276)\n",
      "At idx: [[6.37223739]] -> [[0.33775799]]T vs [[0.47580551]]P\n",
      "At idx: [[4.88750426]] -> [[0.33775799]]T vs [[0.4733065]]P\n",
      "At idx: [[5.73828744]] -> [[0.33775799]]T vs [[0.45168792]]P\n",
      "At idx: [[4.17618157]] -> [[0.84439498]]T vs [[0.46591954]]P\n",
      "At idx: [[5.10429331]] -> [[0.67551599]]T vs [[0.46703758]]P\n",
      "At idx: [[4.05661794]] -> [[0.50663699]]T vs [[0.44400949]]P\n",
      "At idx: [[3.88933506]] -> [[0.50663699]]T vs [[0.45167413]]P\n",
      "At idx: [[4.82082767]] -> [[0.33775799]]T vs [[0.47288024]]P\n",
      "At idx: [[4.3328766]] -> [[0.33775799]]T vs [[0.48305003]]P\n",
      "At idx: [[4.95772573]] -> [[0.84439498]]T vs [[0.47250451]]P\n",
      "Epoch 3 MSE: 0.05670640103241186\n",
      "Min Weight Vals: (-0.02353612347756324, -0.03940214243464389, -0.13283972495020446)\n",
      "Max Weight Vals: (0.11402007826711806, 0.05458186863777189, 0.03200058875980276)\n",
      "Training done in 0.47s!\n"
     ]
    }
   ],
   "source": [
    "# Create ANN\n",
    "ann_model = ANN()\n",
    "ann_model.train(train_set_in, train_set_out, clear_params=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "The following snippet tests the ANN trained previously. It also outputs a plot of the original and predicted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min Weight Vals: (-0.024497876383488288, -0.03547003771274309, -0.14832547521903297)\n",
      "Max Weight Vals: (0.12696981418517392, 0.058081556884464466, 0.022122488807400947)\n",
      "Mean squared error: 0.06957187465895928\n",
      "Test done in 0.0s!\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test workflow\n",
    "# Use a simple validation set for cross-validation\n",
    "#ann_model.test(test_set_in, test_set_out)\n",
    "ann_model.plot_test(test_set_in, test_set_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ee298m",
   "language": "python",
   "name": "ee298m"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
